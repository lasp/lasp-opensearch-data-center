{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the LASP OpenSearch Data Center The LASP OpenSearch Data Center is a collection of CDK constructs that can be used to build and deploy an OpenSearch data center on AWS. This project is designed to provide a scalable and resilient data infrastructure for scientific data analysis and visualization. This documentation provides guidance on using the constructs, the release process, and other relevant information. Getting Started To get started, you can explore the following pages: Release Process Construct Usage Construct Usage The following pages provide detailed information on how to use each of the constructs: Backend Storage Certificate Frontend Frontend Storage Ingest Orchestration Networking OpenSearch","title":"Home"},{"location":"#welcome-to-the-lasp-opensearch-data-center","text":"The LASP OpenSearch Data Center is a collection of CDK constructs that can be used to build and deploy an OpenSearch data center on AWS. This project is designed to provide a scalable and resilient data infrastructure for scientific data analysis and visualization. This documentation provides guidance on using the constructs, the release process, and other relevant information.","title":"Welcome to the LASP OpenSearch Data Center"},{"location":"#getting-started","text":"To get started, you can explore the following pages: Release Process Construct Usage","title":"Getting Started"},{"location":"#construct-usage","text":"The following pages provide detailed information on how to use each of the constructs: Backend Storage Certificate Frontend Frontend Storage Ingest Orchestration Networking OpenSearch","title":"Construct Usage"},{"location":"release_process/","text":"Release Process Releases are automatically created using a GitHub Actions workflow that responds to pushes of annotated git tags. Versioning Version numbers must be PEP440 strings: https://peps.python.org/pep-0440/ That is, [N!]N(.N)*[{a|b|rc}N][.postN][.devN] Preparing for Release Create a release candidate branch. This branch can be named according to the version to be released or it can simply be the last feature branch before releasing a new version. Regardless, this branch is used to polish the release, update the package metadata, etc. The naming convention for release-specific branches is release/X.Y.Z . Bump the version of the package to the version you are about to release, either manually by editing pyproject.toml or by running poetry version X.Y.Z or bumping according to a valid bump rule like poetry version minor (see poetry docs: https://python-poetry.org/docs/cli/#version). Update the version identifier in CITATION.cff . Update changelog.md to reflect that the version is now \"released\" and revisit README.md to keep it up to date. Open a PR to merge the release branch into main. This informs the rest of the team how the release process is progressing as you polish the release branch. You may need to rebase the release branch onto any recent changes to main and resolve any conflicts on a regular basis. When you are satisfied that the release branch is ready, merge the PR into main . This commit should always be a single commit that is a pure fast forward merge (in GitHub it will have to be a rebase and merge because GitHub does not support the fast-forward merge strategy). Check out the main branch, pull the merged changes, and tag the newly created merge commit with the desired version X.Y.Z and push the tag upstream. Automatic Release Process We use GitHub Actions for automatic release process that responds to pushes of git tags. When a tag matching a semantic version ( [0-9]+.[0-9]+.[0-9]+* or test-release/[0-9]+.[0-9]+.[0-9]+* ) is pushed, a workflow runs that builds the package, pushes the artifacts to PyPI or TestPyPI (if tag is prefixed with test-release ), and creates a GitHub Release from the distributed artifacts. Release notes are automatically generated from commit history and the Release name is taken from the basename of the tag. Official Releases Official releases are published to the public PyPI (even if they are release candidates like 1.2.3rc1 ). This differs from test releases, which are only published to TestPyPI and are not published to GitHub at all. If the semantic version has any suffixes (e.g. rc1 ), the release will be marked as a prerelease in GitHub and PyPI. To trigger an official release, push a tag referencing the commit you want to release. The commit MUST be on the main branch. Never publish an official release from a commit that is not present in main ! git checkout main git pull git tag -a X.Y.Z -m \"Version X.Y.Z\" git push origin X.Y.Z Prereleases Unless the pushed tag matches the regex ^[0-9]*\\.[0-9]*\\.[0-9]* , the release will be marked as a prerelease in GitHub. This allows \"official\" prereleases of suffixed tags, e.g. 1.2.3rc4 Release Notes Generation Release notes are generated based on commit messages since the latest non-prerelease Release.","title":"Release Process"},{"location":"release_process/#release-process","text":"Releases are automatically created using a GitHub Actions workflow that responds to pushes of annotated git tags.","title":"Release Process"},{"location":"release_process/#versioning","text":"Version numbers must be PEP440 strings: https://peps.python.org/pep-0440/ That is, [N!]N(.N)*[{a|b|rc}N][.postN][.devN]","title":"Versioning"},{"location":"release_process/#preparing-for-release","text":"Create a release candidate branch. This branch can be named according to the version to be released or it can simply be the last feature branch before releasing a new version. Regardless, this branch is used to polish the release, update the package metadata, etc. The naming convention for release-specific branches is release/X.Y.Z . Bump the version of the package to the version you are about to release, either manually by editing pyproject.toml or by running poetry version X.Y.Z or bumping according to a valid bump rule like poetry version minor (see poetry docs: https://python-poetry.org/docs/cli/#version). Update the version identifier in CITATION.cff . Update changelog.md to reflect that the version is now \"released\" and revisit README.md to keep it up to date. Open a PR to merge the release branch into main. This informs the rest of the team how the release process is progressing as you polish the release branch. You may need to rebase the release branch onto any recent changes to main and resolve any conflicts on a regular basis. When you are satisfied that the release branch is ready, merge the PR into main . This commit should always be a single commit that is a pure fast forward merge (in GitHub it will have to be a rebase and merge because GitHub does not support the fast-forward merge strategy). Check out the main branch, pull the merged changes, and tag the newly created merge commit with the desired version X.Y.Z and push the tag upstream.","title":"Preparing for Release"},{"location":"release_process/#automatic-release-process","text":"We use GitHub Actions for automatic release process that responds to pushes of git tags. When a tag matching a semantic version ( [0-9]+.[0-9]+.[0-9]+* or test-release/[0-9]+.[0-9]+.[0-9]+* ) is pushed, a workflow runs that builds the package, pushes the artifacts to PyPI or TestPyPI (if tag is prefixed with test-release ), and creates a GitHub Release from the distributed artifacts. Release notes are automatically generated from commit history and the Release name is taken from the basename of the tag.","title":"Automatic Release Process"},{"location":"release_process/#official-releases","text":"Official releases are published to the public PyPI (even if they are release candidates like 1.2.3rc1 ). This differs from test releases, which are only published to TestPyPI and are not published to GitHub at all. If the semantic version has any suffixes (e.g. rc1 ), the release will be marked as a prerelease in GitHub and PyPI. To trigger an official release, push a tag referencing the commit you want to release. The commit MUST be on the main branch. Never publish an official release from a commit that is not present in main ! git checkout main git pull git tag -a X.Y.Z -m \"Version X.Y.Z\" git push origin X.Y.Z","title":"Official Releases"},{"location":"release_process/#prereleases","text":"Unless the pushed tag matches the regex ^[0-9]*\\.[0-9]*\\.[0-9]* , the release will be marked as a prerelease in GitHub. This allows \"official\" prereleases of suffixed tags, e.g. 1.2.3rc4","title":"Prereleases"},{"location":"release_process/#release-notes-generation","text":"Release notes are generated based on commit messages since the latest non-prerelease Release.","title":"Release Notes Generation"},{"location":"construct_usage/backend_storage/","text":"Back End Storage Construct Usage This construct provides sensible basic set of storage buckets to provide persistent storage for the back end of the data center, including arrival notification and queueing of those events for later processing. If you wish, you can create these resources independently and pass them to the subsequent constructs that need to know about them. These resources are suggested to be deployed in their own stack so that the rest of the data center can be destroyed without affecting long term data storage. For example, if violated this suggestion and you create the OpenSearch snapshot bucket in the same stack as the OpenSearch Domain, if you ever tear down the domain, the bucket will also be destroyed (or its removal policy will prevent destruction of the Stack itself, leaving it in a failed state). Components Dropbox bucket for handling new files (dropbox pre-processor reads from here) Ingest bucket for storing valid new files (ingest processor reads from here) Opensearch snapshot storage bucket (Opensearch saves index snapshots here) Example Usage in Stack \"\"\"Example Stack for deploying BackendStorage Construct\"\"\" from constructs import Construct from aws_cdk import ( Environment, Stack ) from lasp_opensearch_data_center.constructs.backend_storage import BackendStorageConstruct class BackendStorageStack(Stack): \"\"\"Stack containing resources for persistent back end storage\"\"\" def __init__( self, scope: Construct, construct_id: str, environment: Environment, **kwargs, ) -> None: super().__init__(scope, construct_id, env=environment, **kwargs) self.backendStorage = BackendStorageConstruct( self, \"BackendStorageConstruct\", dropbox_bucket_name=\"dropbox-bucket\", ingest_bucket_name=\"ingest-bucket\", opensearch_snapshot_bucket_name=\"opensearch-snapshot-bucket\", )","title":"Backend Storage"},{"location":"construct_usage/backend_storage/#back-end-storage-construct-usage","text":"This construct provides sensible basic set of storage buckets to provide persistent storage for the back end of the data center, including arrival notification and queueing of those events for later processing. If you wish, you can create these resources independently and pass them to the subsequent constructs that need to know about them. These resources are suggested to be deployed in their own stack so that the rest of the data center can be destroyed without affecting long term data storage. For example, if violated this suggestion and you create the OpenSearch snapshot bucket in the same stack as the OpenSearch Domain, if you ever tear down the domain, the bucket will also be destroyed (or its removal policy will prevent destruction of the Stack itself, leaving it in a failed state).","title":"Back End Storage Construct Usage"},{"location":"construct_usage/backend_storage/#components","text":"Dropbox bucket for handling new files (dropbox pre-processor reads from here) Ingest bucket for storing valid new files (ingest processor reads from here) Opensearch snapshot storage bucket (Opensearch saves index snapshots here)","title":"Components"},{"location":"construct_usage/backend_storage/#example-usage-in-stack","text":"\"\"\"Example Stack for deploying BackendStorage Construct\"\"\" from constructs import Construct from aws_cdk import ( Environment, Stack ) from lasp_opensearch_data_center.constructs.backend_storage import BackendStorageConstruct class BackendStorageStack(Stack): \"\"\"Stack containing resources for persistent back end storage\"\"\" def __init__( self, scope: Construct, construct_id: str, environment: Environment, **kwargs, ) -> None: super().__init__(scope, construct_id, env=environment, **kwargs) self.backendStorage = BackendStorageConstruct( self, \"BackendStorageConstruct\", dropbox_bucket_name=\"dropbox-bucket\", ingest_bucket_name=\"ingest-bucket\", opensearch_snapshot_bucket_name=\"opensearch-snapshot-bucket\", )","title":"Example Usage in Stack"},{"location":"construct_usage/certificate/","text":"Certificate Construct Usage This construct deploys a generic certificate for use throughout a data center back end. NOTE: This requires you to have a Route53 Hosted Zone with your registered domain name in the AWS console. Components SSL certificates used across the back end of the data center Configuration output for cross-stack reference Example Usage in Stack \"\"\"Example Certificate Stack\"\"\" from aws_cdk import ( Stack, Environment, ) from constructs import Construct from lasp_opensearch_data_center.constructs.certificate import CertificateConstruct class CertificateStack(Stack): \"\"\"Manage certificates needed for the Data Center application.\"\"\" def __init__( self, scope: Construct, construct_id: str, domain_name: str, environment: Environment, **kwargs, ) -> None: super().__init__(scope, construct_id, env=environment, **kwargs) certificate = CertificateConstruct( self, \"CertificateConstruct\", domain_name=domain_name ) self.hosted_zone = certificate.hosted_zone self.account_cert = certificate.account_cert","title":"Certificate"},{"location":"construct_usage/certificate/#certificate-construct-usage","text":"This construct deploys a generic certificate for use throughout a data center back end. NOTE: This requires you to have a Route53 Hosted Zone with your registered domain name in the AWS console.","title":"Certificate Construct Usage"},{"location":"construct_usage/certificate/#components","text":"SSL certificates used across the back end of the data center Configuration output for cross-stack reference","title":"Components"},{"location":"construct_usage/certificate/#example-usage-in-stack","text":"\"\"\"Example Certificate Stack\"\"\" from aws_cdk import ( Stack, Environment, ) from constructs import Construct from lasp_opensearch_data_center.constructs.certificate import CertificateConstruct class CertificateStack(Stack): \"\"\"Manage certificates needed for the Data Center application.\"\"\" def __init__( self, scope: Construct, construct_id: str, domain_name: str, environment: Environment, **kwargs, ) -> None: super().__init__(scope, construct_id, env=environment, **kwargs) certificate = CertificateConstruct( self, \"CertificateConstruct\", domain_name=domain_name ) self.hosted_zone = certificate.hosted_zone self.account_cert = certificate.account_cert","title":"Example Usage in Stack"},{"location":"construct_usage/frontend/","text":"Frontend Construct Usage This construct creates resources used to host a front end website on top of the data center back end. Components Policies to allow users in a deployment IAM group to deploy static resources to the front end storage S3 bucket Hosted zone with specified domain name Cloudfront certificate for SSL IP range restriction for access to the website Cloudfront distribution for serving the static site Example Usage in Stack \"\"\"Example front end website stack\"\"\" from aws_cdk import ( Stack, aws_s3 as s3, Environment, ) from constructs import Construct from lasp_opensearch_data_center.constructs.frontend import FrontendConstruct class FrontEndStack(Stack): \"\"\"Example Front End website Stack\"\"\" def __init__( self, scope: Construct, construct_id: str, account_type: str, domain_name: str, frontend_bucket: s3.Bucket, environment: Environment, **kwargs, ) -> None: super().__init__(scope, construct_id, env=environment, **kwargs) self.frontend = FrontendConstruct( self, construct_id=construct_id, environment=environment, account_type=account_type, domain_name=domain_name, frontend_bucket=frontend_bucket, waf_ip_range=\"10.1.1.1/16\" # Custom IP range )","title":"Frontend"},{"location":"construct_usage/frontend/#frontend-construct-usage","text":"This construct creates resources used to host a front end website on top of the data center back end.","title":"Frontend Construct Usage"},{"location":"construct_usage/frontend/#components","text":"Policies to allow users in a deployment IAM group to deploy static resources to the front end storage S3 bucket Hosted zone with specified domain name Cloudfront certificate for SSL IP range restriction for access to the website Cloudfront distribution for serving the static site","title":"Components"},{"location":"construct_usage/frontend/#example-usage-in-stack","text":"\"\"\"Example front end website stack\"\"\" from aws_cdk import ( Stack, aws_s3 as s3, Environment, ) from constructs import Construct from lasp_opensearch_data_center.constructs.frontend import FrontendConstruct class FrontEndStack(Stack): \"\"\"Example Front End website Stack\"\"\" def __init__( self, scope: Construct, construct_id: str, account_type: str, domain_name: str, frontend_bucket: s3.Bucket, environment: Environment, **kwargs, ) -> None: super().__init__(scope, construct_id, env=environment, **kwargs) self.frontend = FrontendConstruct( self, construct_id=construct_id, environment=environment, account_type=account_type, domain_name=domain_name, frontend_bucket=frontend_bucket, waf_ip_range=\"10.1.1.1/16\" # Custom IP range )","title":"Example Usage in Stack"},{"location":"construct_usage/frontend_storage/","text":"FrontendStorage Construct Usage The front end storage construct creates the storage resources for a website for the data center. The idea is that a separate web development team will be creating the static website on top of the OpenSearch API and periodically deploying it to the website S3 bucket. The tag policy prevents anyone else from messing with the website contents. See the FrontendConstruct for the IAM policy and group that gets access to this bucket. Components S3 bucket for storing website contents Tag-based policy to prevent creation and deletion of website contents from S3 bucket Example Usage in Stack \"\"\"Example front end storage stack\"\"\" from aws_cdk import ( Stack, Environment ) from constructs import Construct from lasp_opensearch_data_center.constructs.frontend_storage import FrontendStorageConstruct class FrontendStorageStack(Stack): \"\"\"Example FrontendStorageStack\"\"\" def __init__( self, scope: Construct, domain_name: str, # Pass this in construct_id: str, environment: Environment, **kwargs, ) -> None: super().__init__(scope, construct_id, env=environment, **kwargs) self.frontendStorage = FrontendStorageConstruct( self, construct_id, environment, domain_name=domain_name ) self.frontend_bucket = self.frontendStorage.frontend_bucket","title":"Frontend Storage"},{"location":"construct_usage/frontend_storage/#frontendstorage-construct-usage","text":"The front end storage construct creates the storage resources for a website for the data center. The idea is that a separate web development team will be creating the static website on top of the OpenSearch API and periodically deploying it to the website S3 bucket. The tag policy prevents anyone else from messing with the website contents. See the FrontendConstruct for the IAM policy and group that gets access to this bucket.","title":"FrontendStorage Construct Usage"},{"location":"construct_usage/frontend_storage/#components","text":"S3 bucket for storing website contents Tag-based policy to prevent creation and deletion of website contents from S3 bucket","title":"Components"},{"location":"construct_usage/frontend_storage/#example-usage-in-stack","text":"\"\"\"Example front end storage stack\"\"\" from aws_cdk import ( Stack, Environment ) from constructs import Construct from lasp_opensearch_data_center.constructs.frontend_storage import FrontendStorageConstruct class FrontendStorageStack(Stack): \"\"\"Example FrontendStorageStack\"\"\" def __init__( self, scope: Construct, domain_name: str, # Pass this in construct_id: str, environment: Environment, **kwargs, ) -> None: super().__init__(scope, construct_id, env=environment, **kwargs) self.frontendStorage = FrontendStorageConstruct( self, construct_id, environment, domain_name=domain_name ) self.frontend_bucket = self.frontendStorage.frontend_bucket","title":"Example Usage in Stack"},{"location":"construct_usage/ingest_orchestration/","text":"Ingest Processing Construct Usage The Ingest Processing Construct contains resources uses to orchestrate ingestion of data files into OpenSearch. The actual processing code must be supplied in the form of Lambda functions: Dropbox Lambda and Ingest Lambda. Components Orchestration Queues that pass S3 creation events from the Dropbox Bucket and Ingest Bucket to their respective Lambda functions. These functions are provided with a standard set of environment variables (see constants.py ). Ingest Status Table which can be used to keep track of ingest status, using the functions provided in ingest_status.py for interacting with DynamoDB (or you can write your own DDB code in your Lambda). Optional backup plan for Ingest Status Table (if backup_vault is provided) Configuring Lambda Environment Variables The Dropbox and Ingest Lambda functions must be externally defined by the user of this library and passed in to the IngestProcessingConstruct as configuration objects. Internally, the construct configures the Lambdas with the correct environment variables, standardized in constants.py using the enums DropboxLambdaEnv and IngestLambdaEnv . However, this configuration is only an update to the Lambda environment. You as the user can pass whatever environment variables you want associated with the Lambda and the IngestProcessingConstruct will not overwrite any pre-defined environment variables. It will only add missing variables. Example Usage in Stack \"\"\"Example Stack for OpenSearch deployment using our OpenSearchConstruct\"\"\" from pathlib import Path from constructs import Construct from aws_cdk import ( Duration, Environment, Stack, aws_s3 as s3, aws_opensearchservice as opensearch, aws_lambda as lambda_, aws_ecr_assets as ecr_assets ) from lasp_opensearch_data_center.constructs.ingest_orchestration import IngestProcessingConstruct class IngestStack(Stack): \"\"\"Example Stack for creating ingest orchestration and processing resources\"\"\" def __init__( self, scope: Construct, construct_id: str, environment: Environment, open_search_domain: opensearch.Domain, dropbox_bucket: s3.Bucket, ingest_bucket: s3.Bucket, **kwargs ): super().__init__(scope, construct_id, env=environment, **kwargs) docker_context_path = str((Path(__file__).parent / \"lambda_code_directory_containing_a_Dockerfile\").absolute()) # Create your processing Lambda Functions: Dropbox Lambda and Ingest Lambda # This allows you to customize how these Lambdas validate and process your data. # When they are passed to the IngestProcessingConstruct, they are provided a standard set of environment # variables, so they can access OpenSearch and know the ingest and dropbox bucket names # Environment variable standard names are available in `lasp_opensearch_data_center.constants` self.dropbox_lambda = lambda_.DockerImageFunction( self, \"DropboxLambda\", code=lambda_.DockerImageCode.from_image_asset( directory=docker_context_path, target=\"dropbox-lambda\", # target name in Dockerfile platform=ecr_assets.Platform.LINUX_AMD64 ) ) self.ingest_lambda = lambda_.DockerImageFunction( self, \"IngestLambda\", code=lambda_.DockerImageCode.from_image_asset( directory=docker_context_path, target=\"ingest-lambda\", # target name in Dockerfile platform=ecr_assets.Platform.LINUX_AMD64 ), timeout=Duration.seconds(60 * 15) ) self.ingest_construct = IngestProcessingConstruct( self, \"IngestOrchestration\", open_search_domain=open_search_domain, dropbox_bucket=dropbox_bucket, ingest_bucket=ingest_bucket, dropbox_lambda=self.dropbox_lambda, ingest_lambda=self.ingest_lambda )","title":"Ingest Orchestration"},{"location":"construct_usage/ingest_orchestration/#ingest-processing-construct-usage","text":"The Ingest Processing Construct contains resources uses to orchestrate ingestion of data files into OpenSearch. The actual processing code must be supplied in the form of Lambda functions: Dropbox Lambda and Ingest Lambda.","title":"Ingest Processing Construct Usage"},{"location":"construct_usage/ingest_orchestration/#components","text":"Orchestration Queues that pass S3 creation events from the Dropbox Bucket and Ingest Bucket to their respective Lambda functions. These functions are provided with a standard set of environment variables (see constants.py ). Ingest Status Table which can be used to keep track of ingest status, using the functions provided in ingest_status.py for interacting with DynamoDB (or you can write your own DDB code in your Lambda). Optional backup plan for Ingest Status Table (if backup_vault is provided)","title":"Components"},{"location":"construct_usage/ingest_orchestration/#configuring-lambda-environment-variables","text":"The Dropbox and Ingest Lambda functions must be externally defined by the user of this library and passed in to the IngestProcessingConstruct as configuration objects. Internally, the construct configures the Lambdas with the correct environment variables, standardized in constants.py using the enums DropboxLambdaEnv and IngestLambdaEnv . However, this configuration is only an update to the Lambda environment. You as the user can pass whatever environment variables you want associated with the Lambda and the IngestProcessingConstruct will not overwrite any pre-defined environment variables. It will only add missing variables.","title":"Configuring Lambda Environment Variables"},{"location":"construct_usage/ingest_orchestration/#example-usage-in-stack","text":"\"\"\"Example Stack for OpenSearch deployment using our OpenSearchConstruct\"\"\" from pathlib import Path from constructs import Construct from aws_cdk import ( Duration, Environment, Stack, aws_s3 as s3, aws_opensearchservice as opensearch, aws_lambda as lambda_, aws_ecr_assets as ecr_assets ) from lasp_opensearch_data_center.constructs.ingest_orchestration import IngestProcessingConstruct class IngestStack(Stack): \"\"\"Example Stack for creating ingest orchestration and processing resources\"\"\" def __init__( self, scope: Construct, construct_id: str, environment: Environment, open_search_domain: opensearch.Domain, dropbox_bucket: s3.Bucket, ingest_bucket: s3.Bucket, **kwargs ): super().__init__(scope, construct_id, env=environment, **kwargs) docker_context_path = str((Path(__file__).parent / \"lambda_code_directory_containing_a_Dockerfile\").absolute()) # Create your processing Lambda Functions: Dropbox Lambda and Ingest Lambda # This allows you to customize how these Lambdas validate and process your data. # When they are passed to the IngestProcessingConstruct, they are provided a standard set of environment # variables, so they can access OpenSearch and know the ingest and dropbox bucket names # Environment variable standard names are available in `lasp_opensearch_data_center.constants` self.dropbox_lambda = lambda_.DockerImageFunction( self, \"DropboxLambda\", code=lambda_.DockerImageCode.from_image_asset( directory=docker_context_path, target=\"dropbox-lambda\", # target name in Dockerfile platform=ecr_assets.Platform.LINUX_AMD64 ) ) self.ingest_lambda = lambda_.DockerImageFunction( self, \"IngestLambda\", code=lambda_.DockerImageCode.from_image_asset( directory=docker_context_path, target=\"ingest-lambda\", # target name in Dockerfile platform=ecr_assets.Platform.LINUX_AMD64 ), timeout=Duration.seconds(60 * 15) ) self.ingest_construct = IngestProcessingConstruct( self, \"IngestOrchestration\", open_search_domain=open_search_domain, dropbox_bucket=dropbox_bucket, ingest_bucket=ingest_bucket, dropbox_lambda=self.dropbox_lambda, ingest_lambda=self.ingest_lambda )","title":"Example Usage in Stack"},{"location":"construct_usage/networking/","text":"Networking Construct Usage This is here to provide a working default VPC for running a data center. This VPC should be used across all data center stacks and constructs so everything is in the same network. Components VPC with 1 isolated and 1 public subnet No NAT gateways in the VPC because they are not needed Example Usage in Stack \"\"\"Shared networking resources\"\"\" from constructs import Construct from aws_cdk import ( Environment, Stack ) from lasp_opensearch_data_center.constructs.networking import NetworkingComponentsConstruct class NetworkingComponents(Stack): \"\"\"General networking resources for a Data Center\"\"\" def __init__( self, scope: Construct, construct_id: str, environment: Environment, **kwargs ) -> None: super().__init__(scope, construct_id, env=environment, **kwargs) self.networking = NetworkingComponentsConstruct(self, construct_id) self.vpc = self.networking.vpc","title":"Networking"},{"location":"construct_usage/networking/#networking-construct-usage","text":"This is here to provide a working default VPC for running a data center. This VPC should be used across all data center stacks and constructs so everything is in the same network.","title":"Networking Construct Usage"},{"location":"construct_usage/networking/#components","text":"VPC with 1 isolated and 1 public subnet No NAT gateways in the VPC because they are not needed","title":"Components"},{"location":"construct_usage/networking/#example-usage-in-stack","text":"\"\"\"Shared networking resources\"\"\" from constructs import Construct from aws_cdk import ( Environment, Stack ) from lasp_opensearch_data_center.constructs.networking import NetworkingComponentsConstruct class NetworkingComponents(Stack): \"\"\"General networking resources for a Data Center\"\"\" def __init__( self, scope: Construct, construct_id: str, environment: Environment, **kwargs ) -> None: super().__init__(scope, construct_id, env=environment, **kwargs) self.networking = NetworkingComponentsConstruct(self, construct_id) self.vpc = self.networking.vpc","title":"Example Usage in Stack"},{"location":"construct_usage/opensearch/","text":"OpenSearch Construct Usage The OpenSearch Construct contains all the architecture required for deploying an OpenSearch cluster. It requires some configuration as input (construct arguments) for elements of the architecture that are judged to be the responsibility of the user implementing a CDK application (e.g. a data center). Components OpenSearch Domain (cluster and nodes), with attached access policy for configured IP range Lambda function for taking snapshots of opensearch indexes, scheduled to run daily Example Usage in Stack \"\"\"Example Stack for OpenSearch deployment using our OpenSearchConstruct\"\"\" from constructs import Construct from aws_cdk import ( Environment, Stack, RemovalPolicy, aws_route53 as route53, aws_certificatemanager as acm, aws_s3 as s3, aws_opensearchservice as opensearch ) from lasp_opensearch_data_center.constructs.opensearch import OpenSearchConstruct class OpenSearch(Stack): \"\"\"OpenSearch stack to create the Open Search Domain and cluster nodes NOTE: This stack takes ~20-40 minutes to deploy/destroy the OpenSearch service. Access to the website GUI is available via https://search.{domain_name}/_dashboards/app/home#/ \"\"\" def __init__( self, scope: Construct, construct_id: str, hosted_zone: route53.HostedZone, certificate: acm.Certificate, environment: Environment, snapshot_bucket: s3.Bucket, **kwargs, ) -> None: super().__init__(scope, construct_id, env=environment, **kwargs) self.opensearch = OpenSearchConstruct( self, \"OpensearchConstruct\", hosted_zone=hosted_zone, # Controls URL of OpenSearch API/Dashboards certificate=certificate, environment=environment, opensearch_snapshot_bucket=snapshot_bucket, opensearch_version=opensearch.EngineVersion.open_search(\"2.5\"), # Replace with desired version opensearch_zone_awareness=None, opensearch_data_node_instance_type=\"t3.medium.search\", # Always use *.search instances for opensearch opensearch_data_node_count=1, opensearch_manager_node_instance_type=\"t3.medium.search\", opensearch_manager_node_count=1 opensearch_domain_name=\"opensearch-testing\", # Name of domain, shows up in console opensearch_ip_access_range=\"10.1.1.1/16\", # Replace with custom IP range to control OpenSearch access removal_policy=RemovalPolicy.DESTROY # Modify this as needed for data persistence requirements )","title":"OpenSearch"},{"location":"construct_usage/opensearch/#opensearch-construct-usage","text":"The OpenSearch Construct contains all the architecture required for deploying an OpenSearch cluster. It requires some configuration as input (construct arguments) for elements of the architecture that are judged to be the responsibility of the user implementing a CDK application (e.g. a data center).","title":"OpenSearch Construct Usage"},{"location":"construct_usage/opensearch/#components","text":"OpenSearch Domain (cluster and nodes), with attached access policy for configured IP range Lambda function for taking snapshots of opensearch indexes, scheduled to run daily","title":"Components"},{"location":"construct_usage/opensearch/#example-usage-in-stack","text":"\"\"\"Example Stack for OpenSearch deployment using our OpenSearchConstruct\"\"\" from constructs import Construct from aws_cdk import ( Environment, Stack, RemovalPolicy, aws_route53 as route53, aws_certificatemanager as acm, aws_s3 as s3, aws_opensearchservice as opensearch ) from lasp_opensearch_data_center.constructs.opensearch import OpenSearchConstruct class OpenSearch(Stack): \"\"\"OpenSearch stack to create the Open Search Domain and cluster nodes NOTE: This stack takes ~20-40 minutes to deploy/destroy the OpenSearch service. Access to the website GUI is available via https://search.{domain_name}/_dashboards/app/home#/ \"\"\" def __init__( self, scope: Construct, construct_id: str, hosted_zone: route53.HostedZone, certificate: acm.Certificate, environment: Environment, snapshot_bucket: s3.Bucket, **kwargs, ) -> None: super().__init__(scope, construct_id, env=environment, **kwargs) self.opensearch = OpenSearchConstruct( self, \"OpensearchConstruct\", hosted_zone=hosted_zone, # Controls URL of OpenSearch API/Dashboards certificate=certificate, environment=environment, opensearch_snapshot_bucket=snapshot_bucket, opensearch_version=opensearch.EngineVersion.open_search(\"2.5\"), # Replace with desired version opensearch_zone_awareness=None, opensearch_data_node_instance_type=\"t3.medium.search\", # Always use *.search instances for opensearch opensearch_data_node_count=1, opensearch_manager_node_instance_type=\"t3.medium.search\", opensearch_manager_node_count=1 opensearch_domain_name=\"opensearch-testing\", # Name of domain, shows up in console opensearch_ip_access_range=\"10.1.1.1/16\", # Replace with custom IP range to control OpenSearch access removal_policy=RemovalPolicy.DESTROY # Modify this as needed for data persistence requirements )","title":"Example Usage in Stack"}]}